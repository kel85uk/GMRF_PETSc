0
1
3
5
7
9
11
13
Proc[0]: Received norm from processor 13 
Sample[1]: Expectation = 5.28522692E+00	Variance = 0.00000000E+00	Tol = 5.28522692E+00 
Proc[0]: Received norm from processor 1 
Sample[2]: Expectation = 5.24780385E+00	Variance = 1.40048585E-03	Tol = 2.64621036E-02 
Proc[0]: Received norm from processor 5 
Sample[3]: Expectation = 5.22717923E+00	Variance = 1.78440718E-03	Tol = 2.43885710E-02 
Proc[0]: Received norm from processor 9 
Sample[4]: Expectation = 5.21173505E+00	Variance = 2.05387302E-03	Tol = 2.26598379E-02 
Proc[0]: Received norm from processor 3 
Sample[5]: Expectation = 5.19792245E+00	Variance = 2.40625075E-03	Tol = 2.19374144E-02 
Proc[0]: Received norm from processor 11 
Sample[6]: Expectation = 5.21628594E+00	Variance = 3.69129885E-03	Tol = 2.48035577E-02 
Proc[0]: Received norm from processor 7 
Sample[7]: Expectation = 5.22757134E+00	Variance = 3.92813094E-03	Tol = 2.36888489E-02 
Proc[0]: Received norm from processor 13 
Sample[8]: Expectation = 5.22442650E+00	Variance = 3.50634469E-03	Tol = 2.09354505E-02 
Proc[0]: Received norm from processor 5 
Sample[9]: Expectation = 5.22308339E+00	Variance = 3.13118236E-03	Tol = 1.86523229E-02 
Proc[0]: Received norm from processor 1 
Sample[10]: Expectation = 5.21775989E+00	Variance = 3.07312098E-03	Tol = 1.75303194E-02 
Proc[0]: Received norm from processor 9 
Sample[11]: Expectation = 5.22184055E+00	Variance = 2.96026426E-03	Tol = 1.64047229E-02 
Proc[0]: Received norm from processor 3 
Sample[12]: Expectation = 5.21953596E+00	Variance = 2.77199819E-03	Tol = 1.51986792E-02 
Proc[0]: Received norm from processor 11 
Sample[13]: Expectation = 5.21893564E+00	Variance = 2.56309214E-03	Tol = 1.40414007E-02 
Proc[0]: Received norm from processor 13 
Sample[14]: Expectation = 5.21734106E+00	Variance = 2.41306879E-03	Tol = 1.31286731E-02 
Proc[0]: Received norm from processor 7 
Sample[15]: Expectation = 5.22170955E+00	Variance = 2.51936918E-03	Tol = 1.29598590E-02 
Proc[0]: Received norm from processor 13 
Sample[16]: Expectation = 5.22060398E+00	Variance = 2.38024299E-03	Tol = 1.21969335E-02 
Proc[0]: Received norm from processor 5 
Sample[17]: Expectation = 5.21540438E+00	Variance = 2.67280148E-03	Tol = 1.25388842E-02 
Proc[0]: Received norm from processor 1 
Sample[18]: Expectation = 5.21990359E+00	Variance = 2.86844095E-03	Tol = 1.26237011E-02 
Proc[0]: Received norm from processor 3 
Sample[19]: Expectation = 5.22413072E+00	Variance = 3.03910558E-03	Tol = 1.26472497E-02 
Proc[0]: Received norm from processor 9 
Sample[20]: Expectation = 5.21854066E+00	Variance = 3.48087752E-03	Tol = 1.31925690E-02 
Proc[0]: Received norm from processor 11 
Sample[21]: Expectation = 5.21945490E+00	Variance = 3.33183845E-03	Tol = 1.25959904E-02 
Proc[0]: Received norm from processor 7 
Sample[22]: Expectation = 5.22120698E+00	Variance = 3.24485651E-03	Tol = 1.21446893E-02 
Proc[0]: Received norm from processor 13 
Sample[23]: Expectation = 5.22254909E+00	Variance = 3.14340331E-03	Tol = 1.16905821E-02 
Proc[0]: Received norm from processor 5 
Sample[24]: Expectation = 5.21758342E+00	Variance = 3.57955823E-03	Tol = 1.22126270E-02 
Proc[0]: Received norm from processor 3 
Sample[25]: Expectation = 5.21738900E+00	Variance = 3.43728307E-03	Tol = 1.17256694E-02 
Proc[0]: Received norm from processor 1 
Sample[26]: Expectation = 5.21609970E+00	Variance = 3.34663766E-03	Tol = 1.13453441E-02 
Proc[0]: Received norm from processor 9 
Sample[27]: Expectation = 5.21725904E+00	Variance = 3.25763392E-03	Tol = 1.09842209E-02 
Proc[0]: Received norm from processor 11 
Sample[28]: Expectation = 5.21857260E+00	Variance = 3.18787666E-03	Tol = 1.06701798E-02 
Proc[0]: Received norm from processor 7 
Sample[29]: Expectation = 5.22064101E+00	Variance = 3.19774276E-03	Tol = 1.05008091E-02 
Proc[0]: Received norm from processor 13 
Sample[30]: Expectation = 5.22082406E+00	Variance = 3.09212305E-03	Tol = 1.01523775E-02 
Proc[0]: Received norm from processor 3 
Sample[31]: Expectation = 5.22113007E+00	Variance = 2.99518640E-03	Tol = 9.82949215E-03 
Proc[0]: Received norm from processor 5 
Sample[32]: Expectation = 5.22003030E+00	Variance = 2.93908077E-03	Tol = 9.58364618E-03 
Proc[0]: Received norm from processor 1 
Sample[33]: Expectation = 5.22082549E+00	Variance = 2.87025221E-03	Tol = 9.32616426E-03 
Proc[0]: Received norm from processor 9 
Sample[34]: Expectation = 5.21999674E+00	Variance = 2.80849851E-03	Tol = 9.08861361E-03 
Proc[0]: Received norm from processor 13 
Sample[35]: Expectation = 5.21910597E+00	Variance = 2.75523346E-03	Tol = 8.87248308E-03 
Proc[0]: Received norm from processor 11 
Sample[36]: Expectation = 5.21897041E+00	Variance = 2.67934237E-03	Tol = 8.62706073E-03 
Proc[0]: Received norm from processor 7 
Sample[37]: Expectation = 5.21930348E+00	Variance = 2.61092136E-03	Tol = 8.40032393E-03 
Proc[0]: Received norm from processor 13 
Sample[38]: Expectation = 5.21977900E+00	Variance = 2.55057933E-03	Tol = 8.19271070E-03 
Proc[0]: Received norm from processor 3 
Sample[39]: Expectation = 5.22200328E+00	Variance = 2.67318158E-03	Tol = 8.27907709E-03 
Proc[0]: Received norm from processor 5 
Sample[40]: Expectation = 5.22250801E+00	Variance = 2.61628729E-03	Tol = 8.08747070E-03 
Proc[0]: Received norm from processor 1 
Sample[41]: Expectation = 5.22231253E+00	Variance = 2.55400392E-03	Tol = 7.89257743E-03 
Proc[0]: Received norm from processor 9 
Sample[42]: Expectation = 5.21981412E+00	Variance = 2.74911774E-03	Tol = 8.09043784E-03 
Proc[0]: Received norm from processor 11 
Sample[43]: Expectation = 5.22021059E+00	Variance = 2.69178667E-03	Tol = 7.91199659E-03 
Proc[0]: Received norm from processor 7 
Sample[44]: Expectation = 5.21903599E+00	Variance = 2.68993639E-03	Tol = 7.81888214E-03 
Proc[0]: Received norm from processor 13 
Sample[45]: Expectation = 5.22034875E+00	Variance = 2.70598719E-03	Tol = 7.75455019E-03 
Proc[0]: Received norm from processor 3 
Sample[46]: Expectation = 5.22203192E+00	Variance = 2.77464915E-03	Tol = 7.76649598E-03 
Proc[0]: Received norm from processor 5 
Sample[47]: Expectation = 5.22017935E+00	Variance = 2.87348617E-03	Tol = 7.81907947E-03 
Proc[0]: Received norm from processor 1 
Sample[48]: Expectation = 5.22033512E+00	Variance = 2.81476231E-03	Tol = 7.65773345E-03 
Proc[0]: Received norm from processor 9 
Sample[49]: Expectation = 5.22106532E+00	Variance = 2.78291125E-03	Tol = 7.53618651E-03 
Proc[0]: Received norm from processor 11 
Sample[50]: Expectation = 5.22111788E+00	Variance = 2.72738836E-03	Tol = 7.38564603E-03 
Proc[0]: Received norm from processor 13 
Sample[51]: Expectation = 5.22228663E+00	Variance = 2.74220942E-03	Tol = 7.33272202E-03 
Proc[0]: Received norm from processor 7 
Sample[52]: Expectation = 5.22066397E+00	Variance = 2.82375886E-03	Tol = 7.36906066E-03 
Proc[0]: Received norm from processor 3 
Sample[53]: Expectation = 5.21958049E+00	Variance = 2.83152476E-03	Tol = 7.30924041E-03 
Proc[0]: Received norm from processor 5 
Sample[54]: Expectation = 5.22083843E+00	Variance = 2.86295754E-03	Tol = 7.28132764E-03 
Proc[0]: Received norm from processor 1 
Sample[55]: Expectation = 5.22047699E+00	Variance = 2.81795842E-03	Tol = 7.15790526E-03 
Proc[0]: Received norm from processor 13 
Sample[56]: Expectation = 5.22002366E+00	Variance = 2.77894060E-03	Tol = 7.04442611E-03 
Proc[0]: Received norm from processor 9 
Sample[57]: Expectation = 5.21976192E+00	Variance = 2.73402382E-03	Tol = 6.92570070E-03 
Proc[0]: Received norm from processor 11 
Sample[58]: Expectation = 5.22047218E+00	Variance = 2.71564018E-03	Tol = 6.84261517E-03 
Proc[0]: Received norm from processor 7 
Sample[59]: Expectation = 5.22118858E+00	Variance = 2.69938003E-03	Tol = 6.76403755E-03 
Proc[0]: Received norm from processor 3 
Sample[60]: Expectation = 5.22207780E+00	Variance = 2.70104230E-03	Tol = 6.70949862E-03 
Proc[0]: Received norm from processor 5 
Sample[61]: Expectation = 5.22241428E+00	Variance = 2.66355608E-03	Tol = 6.60793869E-03 
Proc[0]: Received norm from processor 13 
Sample[62]: Expectation = 5.22220546E+00	Variance = 2.62325554E-03	Tol = 6.50465782E-03 
Proc[0]: Received norm from processor 1 
Sample[63]: Expectation = 5.22294026E+00	Variance = 2.61509266E-03	Tol = 6.44277947E-03 
Proc[0]: Received norm from processor 9 
Sample[64]: Expectation = 5.22235548E+00	Variance = 2.59577594E-03	Tol = 6.36859475E-03 
Proc[0]: Received norm from processor 11 
Sample[65]: Expectation = 5.22119120E+00	Variance = 2.64259533E-03	Tol = 6.37615188E-03 
Proc[0]: Received norm from processor 13 
Sample[66]: Expectation = 5.22129756E+00	Variance = 2.60329130E-03	Tol = 6.28043052E-03 
Proc[0]: Received norm from processor 7 
Sample[67]: Expectation = 5.22184482E+00	Variance = 2.58420286E-03	Tol = 6.21049047E-03 
Proc[0]: Received norm from processor 3 
Sample[68]: Expectation = 5.22247306E+00	Variance = 2.57264367E-03	Tol = 6.15085320E-03 
Proc[0]: Received norm from processor 5 
Sample[69]: Expectation = 5.22227592E+00	Variance = 2.53800183E-03	Tol = 6.06486894E-03 
Proc[0]: Received norm from processor 1 
Sample[70]: Expectation = 5.22189274E+00	Variance = 2.51187580E-03	Tol = 5.99032052E-03 
Proc[0]: Received norm from processor 9 
Sample[71]: Expectation = 5.22107687E+00	Variance = 2.52309212E-03	Tol = 5.96125061E-03 
Proc[0]: Received norm from processor 11 
Sample[72]: Expectation = 5.22163889E+00	Variance = 2.51047576E-03	Tol = 5.90488941E-03 
Proc[0]: Received norm from processor 13 
Sample[73]: Expectation = 5.22229613E+00	Variance = 2.50718676E-03	Tol = 5.86046280E-03 
Proc[0]: Received norm from processor 7 
Sample[74]: Expectation = 5.22181675E+00	Variance = 2.49008160E-03	Tol = 5.80084057E-03 
Proc[0]: Received norm from processor 3 
Sample[75]: Expectation = 5.22248620E+00	Variance = 2.49004504E-03	Tol = 5.76199623E-03 
Proc[0]: Received norm from processor 5 
Sample[76]: Expectation = 5.22192680E+00	Variance = 2.48075113E-03	Tol = 5.71327071E-03 
Proc[0]: Received norm from processor 1 
Sample[77]: Expectation = 5.22267520E+00	Variance = 2.49110097E-03	Tol = 5.68787840E-03 
Proc[0]: Received norm from processor 9 
Sample[78]: Expectation = 5.22159505E+00	Variance = 2.54900151E-03	Tol = 5.71659921E-03 
Proc[0]: Received norm from processor 13 
Sample[79]: Expectation = 5.22271937E+00	Variance = 2.61533513E-03	Tol = 5.75373861E-03 
Proc[0]: Received norm from processor 11 
Sample[80]: Expectation = 5.22354066E+00	Variance = 2.63593086E-03	Tol = 5.74013377E-03 
Proc[0]: Received norm from processor 3 
Sample[81]: Expectation = 5.22277786E+00	Variance = 2.64993759E-03	Tol = 5.71972717E-03 
Proc[0]: Received norm from processor 5 
Sample[82]: Expectation = 5.22386384E+00	Variance = 2.71314804E-03	Tol = 5.75214492E-03 
Proc[0]: Received norm from processor 13 
Sample[83]: Expectation = 5.22281197E+00	Variance = 2.77118587E-03	Tol = 5.77821612E-03 
Proc[0]: Received norm from processor 7 
Sample[84]: Expectation = 5.22261726E+00	Variance = 2.74134219E-03	Tol = 5.71270742E-03 
Proc[0]: Received norm from processor 1 
Sample[85]: Expectation = 5.22223091E+00	Variance = 2.72162955E-03	Tol = 5.65854850E-03 
Proc[0]: Received norm from processor 9 
Sample[86]: Expectation = 5.22326045E+00	Variance = 2.78007815E-03	Tol = 5.68563894E-03 
Proc[0]: Received norm from processor 11 
Sample[87]: Expectation = 5.22282598E+00	Variance = 2.76435661E-03	Tol = 5.63686206E-03 
Proc[0]: Received norm from processor 13 
Sample[88]: Expectation = 5.22370444E+00	Variance = 2.80008047E-03	Tol = 5.64084180E-03 
Proc[0]: Received norm from processor 3 
Sample[89]: Expectation = 5.22367547E+00	Variance = 2.76869274E-03	Tol = 5.57753594E-03 
Proc[0]: Received norm from processor 5 
Sample[90]: Expectation = 5.22279892E+00	Variance = 2.80631239E-03	Tol = 5.58401726E-03 
Proc[0]: Received norm from processor 1 
Sample[91]: Expectation = 5.22267062E+00	Variance = 2.77695534E-03	Tol = 5.52412823E-03 
Proc[0]: Received norm from processor 7 
Sample[92]: Expectation = 5.22186200E+00	Variance = 2.80627193E-03	Tol = 5.52294811E-03 
Proc[0]: Received norm from processor 9 
Sample[93]: Expectation = 5.22191881E+00	Variance = 2.77639385E-03	Tol = 5.46385370E-03 
Proc[0]: Received norm from processor 11 
Sample[94]: Expectation = 5.22199231E+00	Variance = 2.74736019E-03	Tol = 5.40622197E-03 
Proc[0]: Received norm from processor 13 
Sample[95]: Expectation = 5.22185319E+00	Variance = 2.72025985E-03	Tol = 5.35110402E-03 
Proc[0]: Received norm from processor 3 
Sample[96]: Expectation = 5.22208472E+00	Variance = 2.69701630E-03	Tol = 5.30036978E-03 
Proc[0]: Received norm from processor 5 
Sample[97]: Expectation = 5.22223785E+00	Variance = 2.67146311E-03	Tol = 5.24793834E-03 
Proc[0]: Received norm from processor 1 
Sample[98]: Expectation = 5.22311948E+00	Variance = 2.71959826E-03	Tol = 5.26792204E-03 
Proc[0]: Received norm from processor 7 
Sample[99]: Expectation = 5.22308374E+00	Variance = 2.69225274E-03	Tol = 5.21483194E-03 
Proc[0]: Received norm from processor 13 
Sample[100]: Expectation = 5.22310094E+00	Variance = 2.66535949E-03	Tol = 5.16271197E-03 
Proc[0]: Received norm from processor 9 
Sample[101]: Expectation = 5.22263784E+00	Variance = 2.66041616E-03	Tol = 5.13232443E-03 
Proc[0]: Sending kill signal to proc 1
Proc[0]: Sending kill signal to proc 3
Proc[0]: Sending kill signal to proc 5
Proc[0]: Sending kill signal to proc 7
Proc[0]: Sending kill signal to proc 9
Proc[0]: Sending kill signal to proc 11
Proc[0]: Sending kill signal to proc 13
Proc[11]: We did 13 samples 
Proc[3]: We did 14 samples 
Proc[5]: We did 14 samples 
Proc[1]: We did 14 samples 
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./Test6 on a linux_gnu_all_cluster_log_C named i10hpc2 with 2 processors, by ug30owag Sun Jul  6 19:51:50 2014
Using Petsc Release Version 3.4.4, unknown 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.911e+01      1.00000   1.911e+01
Objects:              2.240e+02      1.00000   2.240e+02
Flops:                1.133e+09      1.00000   1.133e+09  2.266e+09
Flops/sec:            5.930e+07      1.00000   5.930e+07  1.186e+08
MPI Messages:         3.560e+02      1.04706   3.480e+02  6.960e+02
MPI Message Lengths:  1.230e+06      1.00006   3.534e+03  2.460e+06
MPI Reductions:       9.080e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.1759e-02   0.1%  0.0000e+00   0.0%  5.000e-01   0.1%  2.874e-03        0.0%  0.000e+00   0.0% 
 1: Get options and create all vectors: 3.1497e-02   0.2%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  1.400e+01   1.5% 
 2: Set GMRF operator: 1.2561e-01   0.7%  0.0000e+00   0.0%  4.000e+00   0.6%  6.379e+00        0.2%  1.000e+01   1.1% 
 3: Receive and work on jobs: 1.8941e+01  99.1%  2.2664e+09 100.0%  6.915e+02  99.4%  3.528e+03       99.8%  8.830e+02  97.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


--- Event Stage 1: Get options and create all vectors


--- Event Stage 2: Set GMRF operator

VecSet                 1 1.0 2.8610e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       1 1.0 3.6088e-021002.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0  14  0  0  0 20     0
MatAssemblyEnd         1 1.0 1.7216e-02 1.0 0.00e+00 0.0 4.0e+00 1.1e+03 8.0e+00  0  0  1  0  1  13  0100100 80     0

--- Event Stage 3: Receive and work on jobs

VecMDot              246 1.0 2.5421e-01 1.0 3.14e+08 1.0 0.0e+00 0.0e+00 2.5e+02  1 28  0  0 27   1 28  0  0 28  2472
VecNorm              291 1.0 6.7132e-02 1.0 8.04e+07 1.0 0.0e+00 0.0e+00 2.9e+02  0  7  0  0 32   0  7  0  0 33  2396
VecScale             291 1.0 3.7408e-02 1.0 4.06e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2173
VecCopy               45 1.0 1.8757e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               322 1.0 7.6803e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               30 1.0 8.4527e-03 1.0 8.35e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1977
VecMAXPY             276 1.0 3.0962e-01 1.0 3.83e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2 34  0  0  0   2 34  0  0  0  2471
VecAssemblyBegin      30 1.0 3.7966e-03 4.4 0.00e+00 0.0 0.0e+00 0.0e+00 9.0e+01  0  0  0  0 10   0  0  0  0 10     0
VecAssemblyEnd        30 1.0 4.2677e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterBegin      276 1.0 1.2321e-02 1.0 0.00e+00 0.0 5.5e+02 4.2e+03 0.0e+00  0  0 79 95  0   0  0 80 95  0     0
VecScatterEnd        276 1.0 1.1604e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult              246 1.0 6.6995e-01 1.0 3.07e+08 1.0 4.9e+02 4.2e+03 0.0e+00  4 27 71 84  0   4 27 71 84  0   916
MatConvert            16 1.0 1.5687e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyBegin      15 1.0 1.2145e-0230.8 0.00e+00 0.0 0.0e+00 0.0e+00 3.0e+01  0  0  0  0  3   0  0  0  0  3     0
MatAssemblyEnd        15 1.0 4.0791e-02 1.0 0.00e+00 0.0 4.0e+00 1.0e+03 8.0e+00  0  0  1  0  1   0  0  1  0  1     0
MatGetRowIJ           32 1.0 1.7881e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPGMRESOrthog       246 1.0 5.1576e-01 1.0 6.28e+08 1.0 0.0e+00 0.0e+00 2.5e+02  3 55  0  0 27   3 55  0  0 28  2437
KSPSetUp              16 1.0 1.1612e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              30 1.0 1.7887e+01 1.0 1.13e+09 1.0 4.9e+02 4.2e+03 5.3e+02 94 99 71 84 58  94 99 71 84 60   126
PCSetUp               16 1.0 4.0858e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00 21  0  0  0  1  22  0  0  0  1     0
PCApply              246 1.0 1.2394e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 65  0  0  0  0  65  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector     0             81     88695904     0
      Vector Scatter     0              2         2152     0
              Matrix     0              6     42345424     0
       Krylov Solver     0              2        37904     0
      Preconditioner     0              2         2144     0
              Viewer     1              0            0     0

--- Event Stage 1: Get options and create all vectors

              Vector    19              0            0     0
              Matrix     6              0            0     0
       Krylov Solver     2              0            0     0
      Preconditioner     2              0            0     0

--- Event Stage 2: Set GMRF operator

              Vector     2              1         1552     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         3744     0

--- Event Stage 3: Receive and work on jobs

              Vector    96             35     38002608     0
      Vector Scatter    31             30        32280     0
           Index Set    62             62        49128     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 4.29153e-07
Average time for zero size MPI_Send(): 4.52995e-06
#PETSc Option Table entries:
-Nsamples 100
-TOL 1e-10
-alpha 2
-dim 2
-ksp_type fgmres
-lamb 0.1
-log_summary
-m 500
-mat_type mpiaij
-n 500
-pc_type hypre
-sigma 0.3
-vec_type mpi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Sun Jul  6 15:20:15 2014
Configure options: PETSC_ARCH=linux_gnu_all_cluster_log_C --download-f-blas-lapack --download-hypre --download-superlu_dist --download-parmetis --download-metis --download-suitesparse --download-fftw --download-scalapack --download-mumps --with-mpi-dir=/software/sles/openmpi/1.6.5-ib/ --with-debugging=no --with-shared-libraries=0 --with-mpe-dir=/home/stud/ug30owag/mpe/
-----------------------------------------
Libraries compiled on Sun Jul  6 15:20:15 2014 on i10hpc2 
Machine characteristics: Linux-3.0.101-0.31-default-x86_64-with-SuSE-11-x86_64
Using PETSc directory: /home/stud/ug30owag/petsc
Using PETSc arch: linux_gnu_all_cluster_log_C
-----------------------------------------

Using C compiler: /software/sles/openmpi/1.6.5-ib/bin/mpicc  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -O  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /software/sles/openmpi/1.6.5-ib/bin/mpif90  -Wall -Wno-unused-variable -Wno-unused-dummy-argument -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/mpe/include -I/software/sles/openmpi/1.6.5-ib/include
-----------------------------------------

Using C linker: /software/sles/openmpi/1.6.5-ib/bin/mpicc
Using Fortran linker: /software/sles/openmpi/1.6.5-ib/bin/mpif90
Using libraries: -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lpetsc -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lHYPRE -L/software/sles/openmpi/1.6.5-ib/lib -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc/x86_64-unknown-linux-gnu/4.9.0 -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc -L/auto-home/central-software/sles/gcc/4.9.0/lib64 -L/auto-home/central-software/sles/gcc/4.9.0/lib -lmpi_cxx -lstdc++ -lflapack -lfblas -lX11 -lparmetis -lmetis -lfftw3_mpi -lfftw3 -lpthread -Wl,-rpath,/home/stud/ug30owag/mpe/lib -L/home/stud/ug30owag/mpe/lib -lmpe -lmpi_f90 -lmpi_f77 -lgfortran -lm -lgfortran -lm -lgfortran -lm -lgfortran -lm -lm -lm -lquadmath -lm -lmpi_cxx -lstdc++ -ldl -lmpi -lrt -lnsl -lutil -lgcc_s -lpthread -ldl 
-----------------------------------------

Proc[13]: We did 19 samples 
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./Test6 on a linux_gnu_all_cluster_log_C named i10hpc2 with 3 processors, by ug30owag Sun Jul  6 19:51:50 2014
Using Petsc Release Version 3.4.4, unknown 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.919e+01      1.00000   1.919e+01
Objects:              2.640e+02      1.00000   2.640e+02
Flops:                1.058e+09      1.00034   1.057e+09  3.172e+09
Flops/sec:            5.511e+07      1.00034   5.510e+07  1.653e+08
MPI Messages:         8.080e+02      2.00000   5.457e+02  1.637e+03
MPI Message Lengths:  5.522e+06      2.00000   6.746e+03  1.104e+07
MPI Reductions:       1.221e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.0820e-02   0.1%  0.0000e+00   0.0%  5.000e-01   0.0%  1.222e-03        0.0%  0.000e+00   0.0% 
 1: Get options and create all vectors: 1.9000e-02   0.1%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  1.400e+01   1.1% 
 2: Set GMRF operator: 5.2698e-02   0.3%  0.0000e+00   0.0%  8.000e+00   0.5%  5.425e+00        0.1%  1.000e+01   0.8% 
 3: Receive and work on jobs: 1.9106e+01  99.6%  3.1718e+09 100.0%  1.628e+03  99.5%  6.741e+03       99.9%  1.196e+03  98.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


--- Event Stage 1: Get options and create all vectors

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./Test6 on a linux_gnu_all_cluster_log_C named i10hpc2 with 2 processors, by ug30owag Sun Jul  6 19:51:50 2014
Using Petsc Release Version 3.4.4, unknown 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.919e+01      1.00000   1.919e+01
Objects:              2.160e+02      1.00000   2.160e+02
Flops:                1.060e+09      1.00000   1.060e+09  2.120e+09
Flops/sec:            5.524e+07      1.00000   5.524e+07  1.105e+08
MPI Messages:         3.330e+02      1.04717   3.255e+02  6.510e+02
MPI Message Lengths:  1.150e+06      1.00006   3.532e+03  2.300e+06
MPI Reductions:       8.510e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.8030e-01   1.5%  0.0000e+00   0.0%  5.000e-01   0.1%  3.072e-03        0.0%  0.000e+00   0.0% 
 1: Get options and create all vectors: 2.9523e-02   0.2%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  1.400e+01   1.6% 
 2: Set GMRF operator: 7.2403e-02   0.4%  0.0000e+00   0.0%  4.000e+00   0.6%  6.820e+00        0.2%  1.000e+01   1.2% 
 3: Receive and work on jobs: 1.8809e+01  98.0%  2.1203e+09 100.0%  6.465e+02  99.3%  3.526e+03       99.8%  8.260e+02  97.1% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


--- Event Stage 1: Get options and create all vectors


--- Event Stage 2: Set GMRF operator

VecSet                 1 1.0 5.0068e-06 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 2: Set GMRF operator

VecSet                 1 1.0 4.0531e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       1 1.0 1.8530e-0350.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   1  0  0  0 20     0
MatAssemblyEnd         1 1.0 1.0897e-02 1.0 0.00e+00 0.0 4.0e+00 1.1e+03 8.0e+00  0  0  1  0  1  15  0100100 80     0
MatAssemblyBegin       1 1.0 4.9450e-03133.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   6  0  0  0 20     0
MatAssemblyEnd         1 1.0 8.1511e-03 1.0 0.00e+00 0.0 8.0e+00 1.1e+03 8.0e+00  0  0  0  0  1  15  0100100 80     0

--- Event Stage 3: Receive and work on jobs

VecMDot              230 1.0 2.6596e-01 1.0 2.94e+08 1.0 0.0e+00 0.0e+00 2.3e+02  1 28  0  0 27   1 28  0  0 28  2212
VecNorm              272 1.0 6.7793e-02 1.0 7.52e+07 1.0 0.0e+00 0.0e+00 2.7e+02  0  7  0  0 32   0  7  0  0 33  2218
VecScale             272 1.0 3.5579e-02 1.0 3.80e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2135
VecCopy               42 1.0 1.8677e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               301 1.0 7.5475e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               28 1.0 1.0155e-02 1.0 7.80e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1535
VecMAXPY             258 1.0 3.2500e-01 1.0 3.58e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2 34  0  0  0   2 34  0  0  0  2203
VecAssemblyBegin      28 1.0 2.5792e-03 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 8.4e+01  0  0  0  0 10   0  0  0  0 10     0
VecAssemblyEnd        28 1.0 4.8876e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterBegin      258 1.0 1.3851e-02 1.0 0.00e+00 0.0 5.2e+02 4.2e+03 0.0e+00  0  0 79 94  0   0  0 80 95  0     0
VecScatterEnd        258 1.0 1.2550e-02 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult              230 1.0 6.4591e-01 1.0 2.87e+08 1.0 4.6e+02 4.2e+03 0.0e+00  3 27 71 84  0   3 27 71 84  0   889
MatConvert            15 1.0 1.6294e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyBegin      14 1.0 5.0101e-0294.9 0.00e+00 0.0 0.0e+00 0.0e+00 2.8e+01  0  0  0  0  3   0  0  0  0  3     0
MatAssemblyEnd        14 1.0 3.9920e-02 1.0 0.00e+00 0.0 4.0e+00 1.0e+03 8.0e+00  0  0  1  0  1   0  0  1  0  1     0

--- Event Stage 3: Receive and work on jobs

MatGetRowIJ           30 1.0 1.9312e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              340 1.0 2.6909e-01 1.0 2.97e+08 1.0 0.0e+00 0.0e+00 3.4e+02  1 28  0  0 28   1 28  0  0 28  3315
VecNorm              400 1.0 5.8536e-02 1.1 7.35e+07 1.0 0.0e+00 0.0e+00 4.0e+02  0  7  0  0 33   0  7  0  0 33  3767
VecScale             400 1.0 3.2878e-02 1.0 3.71e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3388
VecCopy               60 1.0 1.6806e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               441 1.0 7.0294e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               40 1.0 8.9409e-03 1.0 7.43e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  2492
VecMAXPY             380 1.0 3.0883e-01 1.0 3.60e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2 34  0  0  0   2 34  0  0  0  3498
KSPGMRESOrthog       230 1.0 5.3577e-01 1.0 5.88e+08 1.0 0.0e+00 0.0e+00 2.3e+02  3 55  0  0 27   3 55  0  0 28  2196
KSPSetUp              15 1.0 1.2563e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyBegin      40 1.0 2.1083e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+02  0  0  0  0 10   0  0  0  0 10     0
VecAssemblyEnd        40 1.0 7.2479e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              28 1.0 1.7438e+01 1.0 1.05e+09 1.0 4.6e+02 4.2e+03 5.0e+02 91 99 71 84 58  93 99 71 84 60   121
PCSetUp               15 1.0 4.0751e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00 21  0  0  0  1  22  0  0  0  1     0
PCApply              230 1.0 1.1947e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 62  0  0  0  0  64  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
VecScatterBegin      380 1.0 1.6158e-02 1.2 0.00e+00 0.0 1.4e+03 6.4e+03 0.0e+00  0  0 88 84  0   0  0 88 84  0     0
VecScatterEnd        380 1.0 2.6223e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector     0             81     88695904     0
      Vector Scatter     0              2         2152     0
              Matrix     0              6     42345424     0
       Krylov Solver     0              2        37904     0
      Preconditioner     0              2         2144     0
              Viewer     1              0            0     0

--- Event Stage 1: Get options and create all vectors

              Vector    19              0            0     0
              Matrix     6              0            0     0
       Krylov Solver     2              0            0     0
      Preconditioner     2              0            0     0

--- Event Stage 2: Set GMRF operator

              Vector     2              1         1552     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         3744     0

--- Event Stage 3: Receive and work on jobs

              Vector    94             33     35766640     0
      Vector Scatter    29             28        30128     0
           Index Set    58             58        46088     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 6.19888e-07
MatMult              340 1.0 6.4449e-01 1.0 2.82e+08 1.0 1.4e+03 4.2e+03 0.0e+00  3 27 83 52  0   3 27 84 52  0  1312
Average time for zero size MPI_Send(): 7.98702e-06
#PETSc Option Table entries:
-Nsamples 100
-TOL 1e-10
-alpha 2
-dim 2
-ksp_type fgmres
-lamb 0.1
-log_summary
-m 500
-mat_type mpiaij
-n 500
-pc_type hypre
-sigma 0.3
-vec_type mpi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Sun Jul  6 15:20:15 2014
Configure options: PETSC_ARCH=linux_gnu_all_cluster_log_C --download-f-blas-lapack --download-hypre --download-superlu_dist --download-parmetis --download-metis --download-suitesparse --download-fftw --download-scalapack --download-mumps --with-mpi-dir=/software/sles/openmpi/1.6.5-ib/ --with-debugging=no --with-shared-libraries=0 --with-mpe-dir=/home/stud/ug30owag/mpe/
-----------------------------------------
Libraries compiled on Sun Jul  6 15:20:15 2014 on i10hpc2 
Machine characteristics: Linux-3.0.101-0.31-default-x86_64-with-SuSE-11-x86_64
Using PETSc directory: /home/stud/ug30owag/petsc
Using PETSc arch: linux_gnu_all_cluster_log_C
-----------------------------------------

Using C compiler: /software/sles/openmpi/1.6.5-ib/bin/mpicc  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -O  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /software/sles/openmpi/1.6.5-ib/bin/mpif90  -Wall -Wno-unused-variable -Wno-unused-dummy-argument -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/mpe/include -I/software/sles/openmpi/1.6.5-ib/include
-----------------------------------------

Using C linker: /software/sles/openmpi/1.6.5-ib/bin/mpicc
Using Fortran linker: /software/sles/openmpi/1.6.5-ib/bin/mpif90
Using libraries: -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lpetsc -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lHYPRE -L/software/sles/openmpi/1.6.5-ib/lib -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc/x86_64-unknown-linux-gnu/4.9.0 -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc -L/auto-home/central-software/sles/gcc/4.9.0/lib64 -L/auto-home/central-software/sles/gcc/4.9.0/lib -lmpi_cxx -lstdc++ -lflapack -lfblas -lX11 -lparmetis -lmetis -lfftw3_mpi -lfftw3 -lpthread -Wl,-rpath,/home/stud/ug30owag/mpe/lib -L/home/stud/ug30owag/mpe/lib -lmpe -lmpi_f90 -lmpi_f77 -lgfortran -lm -lgfortran -lm -lgfortran -lm -lgfortran -lm -lm -lm -lquadmath -lm -lmpi_cxx -lstdc++ -ldl -lmpi -lrt -lnsl -lutil -lgcc_s -lpthread -ldl 
-----------------------------------------

MatConvert            21 1.0 1.7860e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyBegin      20 1.0 1.5893e-0215.9 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+01  0  0  0  0  3   0  0  0  0  3     0
MatAssemblyEnd        20 1.0 4.0305e-02 1.0 0.00e+00 0.0 8.0e+00 1.0e+03 8.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatGetRowIJ           42 1.0 2.9087e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPGMRESOrthog       340 1.0 5.1805e-01 1.0 5.95e+08 1.0 0.0e+00 0.0e+00 3.4e+02  3 56  0  0 28   3 56  0  0 28  3444
KSPSetUp              21 1.0 7.0558e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              40 1.0 1.8039e+01 1.0 1.05e+09 1.0 1.4e+03 4.2e+03 7.3e+02 94 99 83 52 60  94 99 84 52 61   175
PCSetUp               21 1.0 4.7588e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00 25  0  0  0  1  25  0  0  0  1     0
PCApply              340 1.0 1.1917e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 62  0  0  0  0  62  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector     0             81     59175696     0
      Vector Scatter     0              2         2152     0
              Matrix     0              6     28237088     0
       Krylov Solver     0              2        37904     0
      Preconditioner     0              2         2144     0
              Viewer     1              0            0     0

--- Event Stage 1: Get options and create all vectors

              Vector    19              0            0     0
              Matrix     6              0            0     0
       Krylov Solver     2              0            0     0
      Preconditioner     2              0            0     0

--- Event Stage 2: Set GMRF operator

              Vector     2              1         1552     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         3744     0

--- Event Stage 3: Receive and work on jobs

              Vector   106             45     32930000     0
      Vector Scatter    41             40        43040     0
           Index Set    82             82        64328     0
========================================================================================================================
Average time to get PetscTime(): 0
Average time for MPI_Barrier(): 1.7643e-06
Average time for zero size MPI_Send(): 4.6889e-06
#PETSc Option Table entries:
-Nsamples 100
-TOL 1e-10
-alpha 2
-dim 2
-ksp_type fgmres
-lamb 0.1
-log_summary
-m 500
-mat_type mpiaij
-n 500
-pc_type hypre
-sigma 0.3
-vec_type mpi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Sun Jul  6 15:20:15 2014
Configure options: PETSC_ARCH=linux_gnu_all_cluster_log_C --download-f-blas-lapack --download-hypre --download-superlu_dist --download-parmetis --download-metis --download-suitesparse --download-fftw --download-scalapack --download-mumps --with-mpi-dir=/software/sles/openmpi/1.6.5-ib/ --with-debugging=no --with-shared-libraries=0 --with-mpe-dir=/home/stud/ug30owag/mpe/
-----------------------------------------
Libraries compiled on Sun Jul  6 15:20:15 2014 on i10hpc2 
Machine characteristics: Linux-3.0.101-0.31-default-x86_64-with-SuSE-11-x86_64
Using PETSc directory: /home/stud/ug30owag/petsc
Using PETSc arch: linux_gnu_all_cluster_log_C
-----------------------------------------

Using C compiler: /software/sles/openmpi/1.6.5-ib/bin/mpicc  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -O  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /software/sles/openmpi/1.6.5-ib/bin/mpif90  -Wall -Wno-unused-variable -Wno-unused-dummy-argument -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/mpe/include -I/software/sles/openmpi/1.6.5-ib/include
-----------------------------------------

Using C linker: /software/sles/openmpi/1.6.5-ib/bin/mpicc
Using Fortran linker: /software/sles/openmpi/1.6.5-ib/bin/mpif90
Using libraries: -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lpetsc -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lHYPRE -L/software/sles/openmpi/1.6.5-ib/lib -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc/x86_64-unknown-linux-gnu/4.9.0 -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc -L/auto-home/central-software/sles/gcc/4.9.0/lib64 -L/auto-home/central-software/sles/gcc/4.9.0/lib -lmpi_cxx -lstdc++ -lflapack -lfblas -lX11 -lparmetis -lmetis -lfftw3_mpi -lfftw3 -lpthread -Wl,-rpath,/home/stud/ug30owag/mpe/lib -L/home/stud/ug30owag/mpe/lib -lmpe -lmpi_f90 -lmpi_f77 -lgfortran -lm -lgfortran -lm -lgfortran -lm -lgfortran -lm -lm -lm -lquadmath -lm -lmpi_cxx -lstdc++ -ldl -lmpi -lrt -lnsl -lutil -lgcc_s -lpthread -ldl 
-----------------------------------------

Proc[7]: We did 13 samples 
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./Test6 on a linux_gnu_all_cluster_log_C named i10hpc2 with 2 processors, by ug30owag Sun Jul  6 19:51:50 2014
Using Petsc Release Version 3.4.4, unknown 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.938e+01      1.00000   1.938e+01
Objects:              2.240e+02      1.00000   2.240e+02
Flops:                1.114e+09      1.00000   1.114e+09  2.229e+09
Flops/sec:            5.750e+07      1.00000   5.750e+07  1.150e+08
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./Test6 on a linux_gnu_all_cluster_log_C named i10hpc2 with 2 processors, by ug30owag Sun Jul  6 19:51:50 2014
Using Petsc Release Version 3.4.4, unknown 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.938e+01      1.00000   1.938e+01
Objects:              2.240e+02      1.00000   2.240e+02
Flops:                1.127e+09      1.00000   1.127e+09  2.254e+09
Flops/sec:            5.815e+07      1.00000   5.815e+07  1.163e+08
MPI Messages:         3.550e+02      1.04720   3.470e+02  6.940e+02
MPI Message Lengths:  1.226e+06      1.00006   3.533e+03  2.452e+06
MPI Reductions:       9.060e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops
MPI Messages:         3.530e+02      1.04748   3.450e+02  6.900e+02
MPI Message Lengths:  1.218e+06      1.00006   3.530e+03  2.436e+06
MPI Reductions:       9.020e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.6657e-01   0.9%  0.0000e+00   0.0%  5.000e-01   0.1%  2.882e-03        0.0%  0.000e+00   0.0% 
 1: Get options and create all vectors: 3.1092e-02   0.2%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  1.400e+01   1.5% 
 2: Set GMRF operator: 1.2629e-01   0.7%  0.0000e+00   0.0%  4.000e+00   0.6%  6.398e+00        0.2%  1.000e+01   1.1% 
 3: Receive and work on jobs: 1.9057e+01  98.3%  2.2539e+09 100.0%  6.895e+02  99.4%  3.527e+03       99.8%  8.810e+02  97.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.7737e-01   0.9%  0.0000e+00   0.0%  5.000e-01   0.1%  2.899e-03        0.0%  0.000e+00   0.0% 
 1: Get options and create all vectors: 2.2912e-02   0.1%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  1.400e+01   1.6% 
 2: Set GMRF operator: 7.2796e-02   0.4%  0.0000e+00   0.0%  4.000e+00   0.6%  6.435e+00        0.2%  1.000e+01   1.1% 
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

 3: Receive and work on jobs: 1.9108e+01  98.6%  2.2289e+09 100.0%  6.855e+02  99.3%  3.524e+03       99.8%  8.770e+02  97.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


--- Event Stage 1: Get options and create all vectors


--- Event Stage 1: Get options and create all vectors


--- Event Stage 2: Set GMRF operator


--- Event Stage 2: Set GMRF operator

VecSet                 1 1.0 5.0068e-06 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 1 1.0 2.8610e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       1 1.0 3.5512e-02771.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0  14  0  0  0 20     0
MatAssemblyEnd         1 1.0 1.7648e-02 1.0 0.00e+00 0.0 4.0e+00 1.1e+03 8.0e+00  0  0  1  0  1  14  0100100 80     0
MatAssemblyBegin       1 1.0 2.0299e-0353.5 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   1  0  0  0 20     0
MatAssemblyEnd         1 1.0 1.1655e-02 1.0 0.00e+00 0.0 4.0e+00 1.1e+03 8.0e+00  0  0  1  0  1  16  0100100 80     0

--- Event Stage 3: Receive and work on jobs

VecMDot              245 1.0 2.7263e-01 1.0 3.12e+08 1.0 0.0e+00 0.0e+00 2.4e+02  1 28  0  0 27   1 28  0  0 28  2289
VecNorm              290 1.0 6.8292e-02 1.0 8.02e+07 1.0 0.0e+00 0.0e+00 2.9e+02  0  7  0  0 32   0  7  0  0 33  2348

--- Event Stage 3: Receive and work on jobs

VecScale             290 1.0 3.7279e-02 1.0 4.05e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2174
VecCopy               45 1.0 1.8279e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               321 1.0 7.6324e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               30 1.0 9.9487e-03 1.0 8.35e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1679
VecMDot              243 1.0 2.7313e-01 1.0 3.07e+08 1.0 0.0e+00 0.0e+00 2.4e+02  1 28  0  0 27   1 28  0  0 28  2252
VecNorm              288 1.0 7.2284e-02 1.1 7.97e+07 1.0 0.0e+00 0.0e+00 2.9e+02  0  7  0  0 32   0  7  0  0 33  2205
VecMAXPY             275 1.0 3.3457e-01 1.0 3.80e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2 34  0  0  0   2 34  0  0  0  2272
VecAssemblyBegin      30 1.0 3.4339e-03 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 9.0e+01  0  0  0  0 10   0  0  0  0 10     0
VecScale             288 1.0 3.7299e-02 1.0 4.03e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2159
VecCopy               45 1.0 1.7737e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               319 1.0 7.6267e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyEnd        30 1.0 4.3154e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterBegin      275 1.0 1.2643e-02 1.0 0.00e+00 0.0 5.5e+02 4.2e+03 0.0e+00  0  0 79 94  0   0  0 80 95  0     0
VecAXPY               30 1.0 1.0629e-02 1.0 8.35e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1572
VecMAXPY             273 1.0 3.4321e-01 1.0 3.75e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2 34  0  0  0   2 34  0  0  0  2186
VecScatterEnd        275 1.0 1.2662e-02 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyBegin      30 1.0 1.6954e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 9.0e+01  0  0  0  0 10   0  0  0  0 10     0
VecAssemblyEnd        30 1.0 4.3154e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult              245 1.0 6.5871e-01 1.0 3.06e+08 1.0 4.9e+02 4.2e+03 0.0e+00  3 27 71 84  0   3 27 71 84  0   929
VecScatterBegin      273 1.0 1.4063e-02 1.0 0.00e+00 0.0 5.5e+02 4.2e+03 0.0e+00  0  0 79 94  0   0  0 80 95  0     0
VecScatterEnd        273 1.0 1.1403e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult              243 1.0 6.7161e-01 1.0 3.04e+08 1.0 4.9e+02 4.2e+03 0.0e+00  3 27 70 84  0   4 27 71 84  0   904
MatConvert            16 1.0 1.5672e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyBegin      15 1.0 5.7073e-0314.3 0.00e+00 0.0 0.0e+00 0.0e+00 3.0e+01  0  0  0  0  3   0  0  0  0  3     0
MatAssemblyEnd        15 1.0 4.0236e-02 1.0 0.00e+00 0.0 4.0e+00 1.0e+03 8.0e+00  0  0  1  0  1   0  0  1  0  1     0
MatGetRowIJ           32 1.0 1.4544e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatConvert            16 1.0 1.5247e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyBegin      15 1.0 3.3345e-03 2.8 0.00e+00 0.0 0.0e+00 0.0e+00 3.0e+01  0  0  0  0  3   0  0  0  0  3     0
MatAssemblyEnd        15 1.0 4.0753e-02 1.0 0.00e+00 0.0 4.0e+00 1.0e+03 8.0e+00  0  0  1  0  1   0  0  1  0  1     0
MatGetRowIJ           32 1.0 1.9789e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPGMRESOrthog       245 1.0 5.5034e-01 1.0 6.24e+08 1.0 0.0e+00 0.0e+00 2.4e+02  3 55  0  0 27   3 55  0  0 28  2268
KSPSetUp              16 1.0 1.1426e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              30 1.0 1.7849e+01 1.0 1.12e+09 1.0 4.9e+02 4.2e+03 5.3e+02 92 99 71 84 58  94 99 71 84 60   126
PCSetUp               16 1.0 4.0689e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00 21  0  0  0  1  21  0  0  0  1     0
PCApply              245 1.0 1.2339e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 64  0  0  0  0  65  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector     0             81     88695904     0
      Vector Scatter     0              2         2152     0
              Matrix     0              6     42345424     0
       Krylov Solver     0              2        37904     0
      Preconditioner     0              2         2144     0
              Viewer     1              0            0     0

--- Event Stage 1: Get options and create all vectors

KSPGMRESOrthog       243 1.0 5.5572e-01 1.0 6.15e+08 1.0 0.0e+00 0.0e+00 2.4e+02  3 55  0  0 27   3 55  0  0 28  2213
KSPSetUp              16 1.0 1.1424e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
              Vector    19              0            0     0
              Matrix     6              0            0     0
       Krylov Solver     2              0            0     0
      Preconditioner     2              0            0     0

--- Event Stage 2: Set GMRF operator

              Vector     2              1         1552     0
KSPSolve              30 1.0 1.7818e+01 1.0 1.11e+09 1.0 4.9e+02 4.2e+03 5.2e+02 92 99 70 84 58  93 99 71 84 60   124
PCSetUp               16 1.0 4.0814e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00 21  0  0  0  1  21  0  0  0  1     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         3744     0

--- Event Stage 3: Receive and work on jobs

              Vector    96             35     38002608     0
      Vector Scatter    31             30        32280     0
           Index Set    62             62        49128     0
========================================================================================================================
Average time to get PetscTime(): 0
PCApply              243 1.0 1.2273e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 63  0  0  0  0  64  0  0  0  0     0
Average time for MPI_Barrier(): 4.29153e-07
Average time for zero size MPI_Send(): 4.05312e-06
#PETSc Option Table entries:
-Nsamples 100
-TOL 1e-10
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector     0             81     88695904     0
      Vector Scatter     0              2         2152     0
              Matrix     0              6     42345424     0
       Krylov Solver     0              2        37904     0
      Preconditioner     0              2         2144     0
-alpha 2
-dim 2
-ksp_type fgmres
-lamb 0.1
-log_summary
-m 500
-mat_type mpiaij
-n 500
-pc_type hypre
-sigma 0.3
-vec_type mpi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Sun Jul  6 15:20:15 2014
Configure options: PETSC_ARCH=linux_gnu_all_cluster_log_C --download-f-blas-lapack --download-hypre --download-superlu_dist --download-parmetis --download-metis --download-suitesparse --download-fftw --download-scalapack --download-mumps --with-mpi-dir=/software/sles/openmpi/1.6.5-ib/ --with-debugging=no --with-shared-libraries=0 --with-mpe-dir=/home/stud/ug30owag/mpe/              Viewer     1              0            0     0

--- Event Stage 1: Get options and create all vectors

              Vector    19              0            0     0
              Matrix     6              0            0     0
       Krylov Solver     2              0            0     0
      Preconditioner     2              0            0     0

--- Event Stage 2: Set GMRF operator

              Vector     2              1         1552     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         3744     0

--- Event Stage 3: Receive and work on jobs

              Vector    96             35     38002608     0
      Vector Scatter    31             30        32280     0
           Index Set    62             62        49128     0
========================================================================================================================
Average time to get PetscTime(): 2.36988e-05
Average time for MPI_Barrier(): 5.72205e-07
Average time for zero size MPI_Send(): 7.39098e-06
#PETSc Option Table entries:
-Nsamples 100
-TOL 1e-10
-alpha 2
-dim 2
-ksp_type fgmres
-lamb 0.1
-log_summary
-m 500
-mat_type mpiaij
-n 500
-pc_type hypre
-sigma 0.3

-----------------------------------------
Libraries compiled on Sun Jul  6 15:20:15 2014 on i10hpc2 
Machine characteristics: Linux-3.0.101-0.31-default-x86_64-with-SuSE-11-x86_64
Using PETSc directory: /home/stud/ug30owag/petsc
Using PETSc arch: linux_gnu_all_cluster_log_C
-----------------------------------------

Using C compiler: /software/sles/openmpi/1.6.5-ib/bin/mpicc  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -O  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /software/sles/openmpi/1.6.5-ib/bin/mpif90  -Wall -Wno-unused-variable -Wno-unused-dummy-argument -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/mpe/include -I/software/sles/openmpi/1.6.5-ib/include
-----------------------------------------

Using C linker: /software/sles/openmpi/1.6.5-ib/bin/mpicc
Using Fortran linker: /software/sles/openmpi/1.6.5-ib/bin/mpif90
Using libraries: -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lpetsc -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lHYPRE -L/software/sles/openmpi/1.6.5-ib/lib -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc/x86_64-unknown-linux-gnu/4.9.0 -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc -L/auto-home/central-software/sles/gcc/4.9.0/lib64 -L/auto-home/central-software/sles/gcc/4.9.0/lib -lmpi_cxx -lstdc++ -lflapack -lfblas -lX11 -lparmetis -lmetis -lfftw3_mpi -lfftw3 -lpthread -Wl,-rpath,/home/stud/ug30owag/mpe/lib -L/home/stud/ug30owag/mpe/lib -lmpe -lmpi_f90 -lmpi_f77 -lgfortran -lm -lgfortran -lm -lgfortran -lm -lgfortran -lm -lm -lm -lquadmath -lm -lmpi_cxx -lstdc++ -ldl -lmpi -lrt -lnsl -lutil -lgcc_s -lpthread -ldl 
-----------------------------------------

-vec_type mpi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Sun Jul  6 15:20:15 2014
Configure options: PETSC_ARCH=linux_gnu_all_cluster_log_C --download-f-blas-lapack --download-hypre --download-superlu_dist --download-parmetis --download-metis --download-suitesparse --download-fftw --download-scalapack --download-mumps --with-mpi-dir=/software/sles/openmpi/1.6.5-ib/ --with-debugging=no --with-shared-libraries=0 --with-mpe-dir=/home/stud/ug30owag/mpe/
-----------------------------------------
Libraries compiled on Sun Jul  6 15:20:15 2014 on i10hpc2 
Machine characteristics: Linux-3.0.101-0.31-default-x86_64-with-SuSE-11-x86_64
Using PETSc directory: /home/stud/ug30owag/petsc
Using PETSc arch: linux_gnu_all_cluster_log_C
-----------------------------------------

Using C compiler: /software/sles/openmpi/1.6.5-ib/bin/mpicc  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -O  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /software/sles/openmpi/1.6.5-ib/bin/mpif90  -Wall -Wno-unused-variable -Wno-unused-dummy-argument -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/mpe/include -I/software/sles/openmpi/1.6.5-ib/include
-----------------------------------------

Using C linker: /software/sles/openmpi/1.6.5-ib/bin/mpicc
Using Fortran linker: /software/sles/openmpi/1.6.5-ib/bin/mpif90
Using libraries: -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lpetsc -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lHYPRE -L/software/sles/openmpi/1.6.5-ib/lib -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc/x86_64-unknown-linux-gnu/4.9.0 -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc -L/auto-home/central-software/sles/gcc/4.9.0/lib64 -L/auto-home/central-software/sles/gcc/4.9.0/lib -lmpi_cxx -lstdc++ -lflapack -lfblas -lX11 -lparmetis -lmetis -lfftw3_mpi -lfftw3 -lpthread -Wl,-rpath,/home/stud/ug30owag/mpe/lib -L/home/stud/ug30owag/mpe/lib -lmpe -lmpi_f90 -lmpi_f77 -lgfortran -lm -lgfortran -lm -lgfortran -lm -lgfortran -lm -lm -lm -lquadmath -lm -lmpi_cxx -lstdc++ -ldl -lmpi -lrt -lnsl -lutil -lgcc_s -lpthread -ldl 
-----------------------------------------

Proc[9]: We did 14 samples 
Expectation of ||U|| = 5.22263784E+00
Communication GMRF (s) = 0.00000000E+00
Setup GMRF (s) = 3.21175337E-01
Solving GMRF (s) = 8.38722205E+00
Communication SPDE (s) = 1.09801531E-01
Setup SPDE (s) = 8.90561342E-01
Solving SPDE (s) = 1.07210975E+01
Communication root (s) = 1.92581306E+01
Computation root (s) = 1.69110298E-03
Proc[0]: All done! 
Elapsed wall-clock time (sec)= 19.440625 
NGhost = 27 and I am Processor[0] 
tau2 = 0.001247 
kappa = 28.284271 
nu = 1.000000 
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./Test6 on a linux_gnu_all_cluster_log_C named i10hpc2 with 1 processor, by ug30owag Sun Jul  6 19:51:51 2014
Using Petsc Release Version 3.4.4, unknown 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.944e+01      1.00000   1.944e+01
Objects:              3.500e+01      1.00000   3.500e+01
Flops:                0.000e+00      0.00000   0.000e+00  0.000e+00
Flops/sec:            0.000e+00      0.00000   0.000e+00  0.000e+00
MPI Messages:         1.125e+02      1.00000   1.125e+02  1.125e+02
MPI Message Lengths:  4.795e+02      1.00000   4.262e+00  4.795e+02
MPI Reductions:       2.100e+01      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 5.3461e-03   0.0%  0.0000e+00   0.0%  4.500e+00   4.0%  1.600e-01        3.8%  0.000e+00   0.0% 
 1: Get options and create all vectors: 4.3637e-02   0.2%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  1.400e+01  66.7% 
 2: Set GMRF operator: 1.3555e-01   0.7%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  6.000e+00  28.6% 
 3: Start distributing work: 1.9260e+01  99.1%  0.0000e+00   0.0%  1.080e+02  96.0%  4.102e+00       96.2%  0.000e+00   0.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


--- Event Stage 1: Get options and create all vectors


--- Event Stage 2: Set GMRF operator

VecSet                 1 1.0 3.0994e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       1 1.0 2.3842e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0 10   0  0  0  0 33     0
MatAssemblyEnd         1 1.0 1.9852e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0 19  15  0  0  0 67     0

--- Event Stage 3: Start distributing work

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector     0             20     43494912     0
      Vector Scatter     0              1          644     0
              Matrix     0              6     82666820     0
       Krylov Solver     0              2         2704     0
      Preconditioner     0              2         2144     0
              Viewer     1              0            0     0

--- Event Stage 1: Get options and create all vectors

              Vector    19              0            0     0
              Matrix     6              0            0     0
       Krylov Solver     2              0            0     0
      Preconditioner     2              0            0     0

--- Event Stage 2: Set GMRF operator

              Vector     2              1         1552     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         1528     0

--- Event Stage 3: Start distributing work

========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
#PETSc Option Table entries:
-Nsamples 100
-TOL 1e-10
-alpha 2
-dim 2
-ksp_type fgmres
-lamb 0.1
-log_summary
-m 500
-mat_type mpiaij
-n 500
-pc_type hypre
-sigma 0.3
-vec_type mpi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Sun Jul  6 15:20:15 2014
Configure options: PETSC_ARCH=linux_gnu_all_cluster_log_C --download-f-blas-lapack --download-hypre --download-superlu_dist --download-parmetis --download-metis --download-suitesparse --download-fftw --download-scalapack --download-mumps --with-mpi-dir=/software/sles/openmpi/1.6.5-ib/ --with-debugging=no --with-shared-libraries=0 --with-mpe-dir=/home/stud/ug30owag/mpe/
-----------------------------------------
Libraries compiled on Sun Jul  6 15:20:15 2014 on i10hpc2 
Machine characteristics: Linux-3.0.101-0.31-default-x86_64-with-SuSE-11-x86_64
Using PETSc directory: /home/stud/ug30owag/petsc
Using PETSc arch: linux_gnu_all_cluster_log_C
-----------------------------------------

Using C compiler: /software/sles/openmpi/1.6.5-ib/bin/mpicc  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -O  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /software/sles/openmpi/1.6.5-ib/bin/mpif90  -Wall -Wno-unused-variable -Wno-unused-dummy-argument -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/mpe/include -I/software/sles/openmpi/1.6.5-ib/include
-----------------------------------------

Using C linker: /software/sles/openmpi/1.6.5-ib/bin/mpicc
Using Fortran linker: /software/sles/openmpi/1.6.5-ib/bin/mpif90
Using libraries: -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lpetsc -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lHYPRE -L/software/sles/openmpi/1.6.5-ib/lib -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc/x86_64-unknown-linux-gnu/4.9.0 -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc -L/auto-home/central-software/sles/gcc/4.9.0/lib64 -L/auto-home/central-software/sles/gcc/4.9.0/lib -lmpi_cxx -lstdc++ -lflapack -lfblas -lX11 -lparmetis -lmetis -lfftw3_mpi -lfftw3 -lpthread -Wl,-rpath,/home/stud/ug30owag/mpe/lib -L/home/stud/ug30owag/mpe/lib -lmpe -lmpi_f90 -lmpi_f77 -lgfortran -lm -lgfortran -lm -lgfortran -lm -lgfortran -lm -lm -lm -lquadmath -lm -lmpi_cxx -lstdc++ -ldl -lmpi -lrt -lnsl -lutil -lgcc_s -lpthread -ldl 
-----------------------------------------

************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./Test6 on a linux_gnu_all_cluster_log_C named i10hpc2 with 2 processors, by ug30owag Sun Jul  6 19:51:51 2014
Using Petsc Release Version 3.4.4, unknown 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.945e+01      1.00000   1.945e+01
Objects:              2.160e+02      1.00000   2.160e+02
Flops:                1.060e+09      1.00000   1.060e+09  2.120e+09
Flops/sec:            5.450e+07      1.00000   5.450e+07  1.090e+08
MPI Messages:         3.330e+02      1.04717   3.255e+02  6.510e+02
MPI Message Lengths:  1.150e+06      1.00006   3.532e+03  2.300e+06
MPI Reductions:       8.510e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------


Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
./Test6 on a linux_gnu_all_cluster_log_C named i10hpc2 with 2 processors, by ug30owag Sun Jul  6 19:51:51 2014
Using Petsc Release Version 3.4.4, unknown 

                         Max       Max/Min        Avg      Total 
 0:      Main Stage: 4.7647e-02   0.2%  0.0000e+00   0.0%  5.000e-01   0.1%  3.072e-03        0.0%  0.000e+00   0.0% 
 1: Get options and create all vectors: 2.2898e-02   0.1%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  1.400e+01   1.6% 
 2: Set GMRF operator: 7.1004e-02   0.4%  0.0000e+00   0.0%  4.000e+00   0.6%  6.820e+00        0.2%  1.000e+01   1.2% 
Time (sec):           1.945e+01      1.00000   1.945e+01
Objects:              2.240e+02      1.00000   2.240e+02
Flops:                1.114e+09      1.00000   1.114e+09  2.229e+09
 3: Receive and work on jobs: 1.9311e+01  99.3%  2.1203e+09 100.0%  6.465e+02  99.3%  3.526e+03       99.8%  8.260e+02  97.1% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
Flops/sec:            5.729e+07      1.00000   5.729e+07  1.146e+08
MPI Messages:         3.530e+02      1.04748   3.450e+02  6.900e+02
MPI Message Lengths:  1.218e+06      1.00006   3.530e+03  2.436e+06
MPI Reductions:       9.020e+02      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.1984e-02   0.1%  0.0000e+00   0.0%  5.000e-01   0.1%  2.899e-03        0.0%  0.000e+00   0.0% 
 1: Get options and create all vectors: 2.3615e-02   0.1%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  1.400e+01   1.6% 
 2: Set GMRF operator: 7.1409e-02   0.4%  0.0000e+00   0.0%  4.000e+00   0.6%  6.435e+00        0.2%  1.000e+01   1.1% 
 3: Receive and work on jobs: 1.9345e+01  99.4%  2.2289e+09 100.0%  6.855e+02  99.3%  3.524e+03       99.8%  8.770e+02  97.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage


--- Event Stage 1: Get options and create all vectors


--- Event Stage 1: Get options and create all vectors


--- Event Stage 2: Set GMRF operator

VecSet                 1 1.0 3.0994e-06 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       1 1.0 3.3808e-04 9.9 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0 20     0
MatAssemblyEnd         1 1.0 1.1947e-02 1.0 0.00e+00 0.0 4.0e+00 1.1e+03 8.0e+00  0  0  1  0  1  16  0100100 80     0

--- Event Stage 3: Receive and work on jobs

VecMDot              243 1.0 2.7864e-01 1.0 3.07e+08 1.0 0.0e+00 0.0e+00 2.4e+02  1 28  0  0 27   1 28  0  0 28  2207
VecNorm              288 1.0 7.2699e-02 1.0 7.97e+07 1.0 0.0e+00 0.0e+00 2.9e+02  0  7  0  0 32   0  7  0  0 33  2192
VecScale             288 1.0 3.7883e-02 1.0 4.03e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2126
VecCopy               45 1.0 1.9421e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               319 1.0 7.7688e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               30 1.0 1.2352e-02 1.0 8.35e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1353
VecMAXPY             273 1.0 3.4614e-01 1.0 3.75e+08 1.0 0.0e+00 0.0e+00 0.0e+00  2 34  0  0  0   2 34  0  0  0  2167
VecAssemblyBegin      30 1.0 2.3215e-03 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 9.0e+01  0  0  0  0 10   0  0  0  0 10     0
VecAssemblyEnd        30 1.0 5.3406e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterBegin      273 1.0 1.6862e-02 1.0 0.00e+00 0.0 5.5e+02 4.2e+03 0.0e+00  0  0 79 94  0   0  0 80 95  0     0
VecScatterEnd        273 1.0 1.3403e-02 2.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult              243 1.0 6.6268e-01 1.0 3.04e+08 1.0 4.9e+02 4.2e+03 0.0e+00  3 27 70 84  0   3 27 71 84  0   916
MatConvert            16 1.0 1.8310e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyBegin      15 1.0 5.0826e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 3.0e+01  0  0  0  0  3   0  0  0  0  3     0
MatAssemblyEnd        15 1.0 4.5306e-02 1.0 0.00e+00 0.0 4.0e+00 1.0e+03 8.0e+00  0  0  1  0  1   0  0  1  0  1     0

--- Event Stage 2: Set GMRF operator

MatGetRowIJ           32 1.0 2.5749e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 1 1.0 4.0531e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPGMRESOrthog       243 1.0 5.6104e-01 1.0 6.15e+08 1.0 0.0e+00 0.0e+00 2.4e+02  3 55  0  0 27   3 55  0  0 28  2192
KSPSetUp              16 1.0 1.2558e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              30 1.0 1.8176e+01 1.0 1.11e+09 1.0 4.9e+02 4.2e+03 5.2e+02 93 99 70 84 58  94 99 71 84 60   122
PCSetUp               16 1.0 4.3562e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00 22  0  0  0  1  23  0  0  0  1     0
PCApply              243 1.0 1.2345e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 63  0  0  0  0  64  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector     0             81     88695904     0
      Vector Scatter     0              2         2152     0
              Matrix     0              6     42345424     0
       Krylov Solver     0              2        37904     0
      Preconditioner     0              2         2144     0
              Viewer     1              0            0     0

--- Event Stage 1: Get options and create all vectors

              Vector    19              0            0     0
              Matrix     6              0            0     0
       Krylov Solver     2              0            0     0
      Preconditioner     2              0            0     0

--- Event Stage 2: Set GMRF operator

              Vector     2              1         1552     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         3744     0

--- Event Stage 3: Receive and work on jobs

              Vector    96             35     38002608     0
      Vector Scatter    31             30        32280     0
           Index Set    62             62        49128     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.8147e-07
Average time for zero size MPI_Send(): 4.52995e-06
#PETSc Option Table entries:
-Nsamples 100
-TOL 1e-10
-alpha 2
-dim 2
-ksp_type fgmres
-lamb 0.1
-log_summary
-m 500
-mat_type mpiaij
-n 500
-pc_type hypre
-sigma 0.3
-vec_type mpi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Sun Jul  6 15:20:15 2014
Configure options: PETSC_ARCH=linux_gnu_all_cluster_log_C --download-f-blas-lapack --download-hypre --download-superlu_dist --download-parmetis --download-metis --download-suitesparse --download-fftw --download-scalapack --download-mumps --with-mpi-dir=/software/sles/openmpi/1.6.5-ib/ --with-debugging=no --with-shared-libraries=0 --with-mpe-dir=/home/stud/ug30owag/mpe/
-----------------------------------------
Libraries compiled on Sun Jul  6 15:20:15 2014 on i10hpc2 
Machine characteristics: Linux-3.0.101-0.31-default-x86_64-with-SuSE-11-x86_64
Using PETSc directory: /home/stud/ug30owag/petsc
Using PETSc arch: linux_gnu_all_cluster_log_C
-----------------------------------------

Using C compiler: /software/sles/openmpi/1.6.5-ib/bin/mpicc  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -O  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /software/sles/openmpi/1.6.5-ib/bin/mpif90  -Wall -Wno-unused-variable -Wno-unused-dummy-argument -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/mpe/include -I/software/sles/openmpi/1.6.5-ib/include
-----------------------------------------
MatAssemblyBegin       1 1.0 8.6284e-0421.7 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   1  0  0  0 20     0

Using C linker: /software/sles/openmpi/1.6.5-ib/bin/mpicc
Using Fortran linker: /software/sles/openmpi/1.6.5-ib/bin/mpif90
Using libraries: -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lpetsc -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lHYPRE -L/software/sles/openmpi/1.6.5-ib/lib -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc/x86_64-unknown-linux-gnu/4.9.0 -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc -L/auto-home/central-software/sles/gcc/4.9.0/lib64 -L/auto-home/central-software/sles/gcc/4.9.0/lib -lmpi_cxx -lstdc++ -lflapack -lfblas -lX11 -lparmetis -lmetis -lfftw3_mpi -lfftw3 -lpthread -Wl,-rpath,/home/stud/ug30owag/mpe/lib -L/home/stud/ug30owag/mpe/lib -lmpe -lmpi_f90 -lmpi_f77 -lgfortran -lm -lgfortran -lm -lgfortran -lm -lgfortran -lm -lm -lm -lquadmath -lm -lmpi_cxx -lstdc++ -ldl -lmpi -lrt -lnsl -lutil -lgcc_s -lpthread -ldl 
-----------------------------------------

MatAssemblyEnd         1 1.0 1.1473e-02 1.0 0.00e+00 0.0 4.0e+00 1.1e+03 8.0e+00  0  0  1  0  1  16  0100100 80     0

--- Event Stage 3: Receive and work on jobs

VecMDot              230 1.0 2.9051e-01 1.0 2.94e+08 1.0 0.0e+00 0.0e+00 2.3e+02  1 28  0  0 27   1 28  0  0 28  2025
VecNorm              272 1.0 1.1964e-01 1.1 7.52e+07 1.0 0.0e+00 0.0e+00 2.7e+02  1  7  0  0 32   1  7  0  0 33  1257
VecScale             272 1.0 4.1714e-02 1.0 3.80e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  1821
VecCopy               42 1.0 1.8381e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               301 1.0 7.5261e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               28 1.0 8.7852e-03 1.0 7.80e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  1775
VecMAXPY             258 1.0 2.8742e-01 1.1 3.58e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1 34  0  0  0   1 34  0  0  0  2492
VecAssemblyBegin      28 1.0 5.3101e-03 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 8.4e+01  0  0  0  0 10   0  0  0  0 10     0
VecAssemblyEnd        28 1.0 5.9843e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterBegin      258 1.0 1.4531e-02 1.0 0.00e+00 0.0 5.2e+02 4.2e+03 0.0e+00  0  0 79 94  0   0  0 80 95  0     0
VecScatterEnd        258 1.0 4.0878e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult              230 1.0 6.3465e-01 1.0 2.87e+08 1.0 4.6e+02 4.2e+03 0.0e+00  3 27 71 84  0   3 27 71 84  0   904
MatConvert            15 1.0 1.8866e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyBegin      14 1.0 3.2283e-0245.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.8e+01  0  0  0  0  3   0  0  0  0  3     0
MatAssemblyEnd        14 1.0 4.2062e-02 1.0 0.00e+00 0.0 4.0e+00 1.0e+03 8.0e+00  0  0  1  0  1   0  0  1  0  1     0
MatGetRowIJ           30 1.0 1.9789e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPGMRESOrthog       230 1.0 5.2305e-01 1.0 5.88e+08 1.0 0.0e+00 0.0e+00 2.3e+02  3 55  0  0 27   3 55  0  0 28  2250
KSPSetUp              15 1.0 1.3072e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              28 1.0 1.8153e+01 1.0 1.05e+09 1.0 4.6e+02 4.2e+03 5.0e+02 93 99 71 84 58  94 99 71 84 60   116
PCSetUp               15 1.0 4.3958e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 8.0e+00 23  0  0  0  1  23  0  0  0  1     0
PCApply              230 1.0 1.2328e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 63  0  0  0  0  64  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector     0             81     88695904     0
      Vector Scatter     0              2         2152     0
              Matrix     0              6     42345424     0
       Krylov Solver     0              2        37904     0
      Preconditioner     0              2         2144     0
              Viewer     1              0            0     0

--- Event Stage 1: Get options and create all vectors

              Vector    19              0            0     0
              Matrix     6              0            0     0
       Krylov Solver     2              0            0     0
      Preconditioner     2              0            0     0

--- Event Stage 2: Set GMRF operator

              Vector     2              1         1552     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         3744     0

--- Event Stage 3: Receive and work on jobs

              Vector    94             33     35766640     0
      Vector Scatter    29             28        30128     0
           Index Set    58             58        46088     0
========================================================================================================================
Average time to get PetscTime(): 0
Average time for MPI_Barrier(): 8.10623e-07
Average time for zero size MPI_Send(): 2.98023e-06
#PETSc Option Table entries:
-Nsamples 100
-TOL 1e-10
-alpha 2
-dim 2
-ksp_type fgmres
-lamb 0.1
-log_summary
-m 500
-mat_type mpiaij
-n 500
-pc_type hypre
-sigma 0.3
-vec_type mpi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Sun Jul  6 15:20:15 2014
Configure options: PETSC_ARCH=linux_gnu_all_cluster_log_C --download-f-blas-lapack --download-hypre --download-superlu_dist --download-parmetis --download-metis --download-suitesparse --download-fftw --download-scalapack --download-mumps --with-mpi-dir=/software/sles/openmpi/1.6.5-ib/ --with-debugging=no --with-shared-libraries=0 --with-mpe-dir=/home/stud/ug30owag/mpe/
-----------------------------------------
Libraries compiled on Sun Jul  6 15:20:15 2014 on i10hpc2 
Machine characteristics: Linux-3.0.101-0.31-default-x86_64-with-SuSE-11-x86_64
Using PETSc directory: /home/stud/ug30owag/petsc
Using PETSc arch: linux_gnu_all_cluster_log_C
-----------------------------------------

Using C compiler: /software/sles/openmpi/1.6.5-ib/bin/mpicc  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -O  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /software/sles/openmpi/1.6.5-ib/bin/mpif90  -Wall -Wno-unused-variable -Wno-unused-dummy-argument -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/include -I/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/include -I/home/stud/ug30owag/mpe/include -I/software/sles/openmpi/1.6.5-ib/include
-----------------------------------------

Using C linker: /software/sles/openmpi/1.6.5-ib/bin/mpicc
Using Fortran linker: /software/sles/openmpi/1.6.5-ib/bin/mpif90
Using libraries: -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lpetsc -Wl,-rpath,/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -L/home/stud/ug30owag/petsc/linux_gnu_all_cluster_log_C/lib -lsuperlu_dist_3.3 -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lHYPRE -L/software/sles/openmpi/1.6.5-ib/lib -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc/x86_64-unknown-linux-gnu/4.9.0 -L/auto-home/central-software/sles/gcc/4.9.0/lib/gcc -L/auto-home/central-software/sles/gcc/4.9.0/lib64 -L/auto-home/central-software/sles/gcc/4.9.0/lib -lmpi_cxx -lstdc++ -lflapack -lfblas -lX11 -lparmetis -lmetis -lfftw3_mpi -lfftw3 -lpthread -Wl,-rpath,/home/stud/ug30owag/mpe/lib -L/home/stud/ug30owag/mpe/lib -lmpe -lmpi_f90 -lmpi_f77 -lgfortran -lm -lgfortran -lm -lgfortran -lm -lgfortran -lm -lm -lm -lquadmath -lm -lmpi_cxx -lstdc++ -ldl -lmpi -lrt -lnsl -lutil -lgcc_s -lpthread -ldl 
-----------------------------------------

